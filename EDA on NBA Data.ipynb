{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793b62d9",
   "metadata": {},
   "source": [
    "<h1>NBA correlation problem</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f6af56-1e3a-4954-aebf-20c34b2be8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "/tmp/ipykernel_40352/3548313310.py:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "  cities.loc[:, sport] = cities.loc[:, sport].replace(\"\\[.*\\]\", \"\", regex=True)\n"
     ]
    }
   ],
   "source": [
    "def nba_correlation(): \n",
    "    # Importing necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    import re\n",
    "    \n",
    "    # Importing NBA data and city data\n",
    "    nba_df = pd.read_csv(\"assets/nba.csv\")  # Loading NBA data from a CSV file\n",
    "    cities = pd.read_html(\"assets/wikipedia_data.html\")[1]  # Loading city data from an HTML file (second table)\n",
    "    cities = cities.iloc[:-1, [0, 3, 5, 6, 7, 8]]  # Selecting relevant columns and removing the last row\n",
    "    cities.head()  # Displaying the first 5 rows of the city data\n",
    "\n",
    "    # Defining a function to clean up city data and extract the relevant information for NBA teams \n",
    "    # This function was made by the same EDA done on other sports so I just made a function that does the same for all sports\n",
    "    def clean_up_cities_rules(cities, sport): \n",
    "        # Renaming columns to easier-to-work-with names\n",
    "        cities.rename(columns = {\"Population (2016 est.)[8]\":\"Population\", \"Metropolitan area\":\"City\"}, inplace=True)\n",
    "        # Keeping only the relevant columns for the sport (i.e., city, population, and NBA info)\n",
    "        cities = cities[[\"City\", \"Population\", sport]]\n",
    "        # Replacing any '--' values with NaN, making it easier to handle missing values\n",
    "        cities.loc[:, sport] = cities.loc[:, sport].replace(\"—\", np.nan)\n",
    "        cities = cities.dropna()  # Dropping rows with missing values\n",
    "        # Removing any extra information within square brackets in the sport column (e.g., team associations)\n",
    "        cities.loc[:, sport] = cities.loc[:, sport].replace(\"\\[.*\\]\", \"\", regex=True)\n",
    "        # Replacing empty strings with NaN (empty values in the sport column imply no NBA team in that city)\n",
    "        cities.loc[:, sport] = cities.loc[:, sport].replace(\"\", np.nan)\n",
    "        cities = cities.dropna()  # Dropping rows with missing values again\n",
    "        return cities\n",
    "\n",
    "    # Cleaning up city data specifically for NBA\n",
    "    metro_rules_NBA = clean_up_cities_rules(cities, \"NBA\")\n",
    "\n",
    "    # Loading NBA statistics data\n",
    "    nba_stats = pd.read_csv(\"assets/nba.csv\")\n",
    "\n",
    "    # Cleaning the team names (removing unnecessary characters)\n",
    "    def clean_nba_name(row): \n",
    "        # Removing everything after an asterisk (indicating additional info about teams)\n",
    "        row = re.sub(r'\\*.*', \"\", row).strip()\n",
    "        # Removing everything inside parentheses (e.g., \"(West)\" or \"(East)\")\n",
    "        row = re.sub(r\"\\(.*\", \"\", row).strip()\n",
    "        return row\n",
    "\n",
    "    # Applying the cleaning function to the 'team' column\n",
    "    nba_stats.loc[:, \"team\"] = nba_stats.loc[:, \"team\"].apply(lambda x: clean_nba_name(x))\n",
    "\n",
    "    # Filtering the dataset to keep only the data for 2018\n",
    "    mask2018 = nba_stats[\"year\"] == 2018\n",
    "    nba_stats = nba_stats[mask2018]\n",
    "\n",
    "    # Keeping only the team name and their win/loss percentage\n",
    "    nba_stats = nba_stats.loc[:, [\"team\", \"W/L%\"]]\n",
    "    # Adding a new column for city names (initially empty)\n",
    "    nba_stats[\"City\"] = np.nan\n",
    "\n",
    "    # We are now going to handle teams that have problematic city names\n",
    "    easy_named_teams = []  # List of teams that we can match easily with the city data\n",
    "    problematic_names = []  # List of teams with city names that are harder to match\n",
    "\n",
    "    # Function to check if the team name matches any city name in the city data\n",
    "    def assign_city(tname): \n",
    "        '''\n",
    "        The thinking is quite elegant here to go through all the names really easy \n",
    "        1) Get the the team-name and find its first word. If it is a city then put it on the easy ones\n",
    "        2) If not put it on the problematic\n",
    "\n",
    "        Then 1) solves most and 2) will just be done manually (big saving on if we were to do it for 4 dataframes manually).\n",
    "        Lastly we just go over it with the eye to see if there is anything we missed e.g. New York ~ New Jersey \n",
    "        '''\n",
    "        if tname.split()[0] in str(metro_rules_NBA[\"City\"].unique()):\n",
    "            easy_named_teams.append(tname)\n",
    "        else: \n",
    "            problematic_names.append(tname)\n",
    "\n",
    "    # Applying this function to the 'team' column to assign cities\n",
    "    nba_stats[\"team\"].apply(lambda x: assign_city(x))\n",
    "\n",
    "    # We create a mapping dictionary for teams with problematic names - this is the 2) of the function\n",
    "    problematic_mapping = {\n",
    "        \"Brooklyn Nets\": \"New York City\",\n",
    "        \"Golden State Warriors\": \"San Francisco Bay Area\",\n",
    "        \"Utah Jazz\": \"Salt Lake City\",\n",
    "        \"Minnesota Timberwolves\": \"Minneapolis–Saint Paul\",\n",
    "    }\n",
    "\n",
    "    # Function to assign a city to a team based on the mapping dictionary\n",
    "    # We will use it to map both the easy and the problematic then\n",
    "    def team_rename(row, dictionary): \n",
    "        team_name = row[\"team\"]  # Get the team name\n",
    "        if team_name in dictionary.keys():  # If the team is in the mapping dictionary\n",
    "            row[\"City\"] = dictionary[team_name]  # Assign the corresponding city name\n",
    "        return row\n",
    "\n",
    "    # Applying the problematic mapping to the NBA stats\n",
    "    nba_stats = nba_stats.apply(team_rename, axis=1, dictionary=problematic_mapping)\n",
    "\n",
    "    # Handling the easy-to-rename teams (those with city names that match the first word of the team name)\n",
    "    def find_city_by_team(tname):\n",
    "        result = metro_rules_NBA[metro_rules_NBA[\"City\"].str.startswith(tname.split()[0])]\n",
    "        return result[\"City\"].iloc[0]  # Return the city name that matches the team's first word\n",
    "\n",
    "    easy_named_teams_dict = {}  # Dictionary to store city names for easy-to-match teams\n",
    "\n",
    "    # Now for the easy - let's first make the mapping dictionary\n",
    "    for team in easy_named_teams: \n",
    "        try: \n",
    "            easy_named_teams_dict[team] = find_city_by_team(team)\n",
    "        except: \n",
    "            continue  # Skip if no match is found\n",
    "\n",
    "    # Applying the city names for easy teams\n",
    "    nba_stats = nba_stats.apply(team_rename, axis=1, dictionary=easy_named_teams_dict)\n",
    "\n",
    "    # Correcting the names of teams with known mismatches (San Antonio and New Orleans)\n",
    "    nba_stats.loc[20, \"City\"] = \"New Orleans\"\n",
    "    nba_stats.loc[21, \"City\"] = \"San Antonio\"\n",
    "\n",
    "    # Converting the 'W/L%' column to a float type for analysis\n",
    "    nba_stats[\"W/L%\"] = nba_stats[\"W/L%\"].astype(float)\n",
    "\n",
    "    # Calculating the average win/loss percentage by city\n",
    "    mean_wl = nba_stats[\"W/L%\"].groupby(nba_stats[\"City\"]).mean()\n",
    "\n",
    "    # Merging the NBA win/loss data with city population data based on the city column\n",
    "    final = pd.merge(metro_rules_NBA, mean_wl, how=\"inner\", on=\"City\")\n",
    "\n",
    "    # Calculating the Pearson correlation between city population and NBA win/loss percentage\n",
    "    correlation = stats.pearsonr(final[\"Population\"].astype(float), final[\"W/L%\"].astype(float))\n",
    "\n",
    "    # Returning the correlation value\n",
    "    return correlation[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fb5d80-46da-4c7a-a181-32c4a2a24228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40352/3548313310.py:25: SyntaxWarning: invalid escape sequence '\\['\n",
      "  cities.loc[:, sport] = cities.loc[:, sport].replace(\"\\[.*\\]\", \"\", regex=True)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'lxml'.  Use pip or conda to install lxml.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Neural-Playground/.venv/lib/python3.12/site-packages/pandas/compat/_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1310\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lxml'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnba_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mnba_correlation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Importing NBA data and city data\u001b[39;00m\n\u001b[32m      9\u001b[39m nba_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33massets/nba.csv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Loading NBA data from a CSV file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m cities = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massets/wikipedia_data.html\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# Loading city data from an HTML file (second table)\u001b[39;00m\n\u001b[32m     11\u001b[39m cities = cities.iloc[:-\u001b[32m1\u001b[39m, [\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m8\u001b[39m]]  \u001b[38;5;66;03m# Selecting relevant columns and removing the last row\u001b[39;00m\n\u001b[32m     12\u001b[39m cities.head()  \u001b[38;5;66;03m# Displaying the first 5 rows of the city data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Neural-Playground/.venv/lib/python3.12/site-packages/pandas/io/html.py:1240\u001b[39m, in \u001b[36mread_html\u001b[39m\u001b[34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1225\u001b[39m     [\n\u001b[32m   1226\u001b[39m         is_file_like(io),\n\u001b[32m   (...)\u001b[39m\u001b[32m   1230\u001b[39m     ]\n\u001b[32m   1231\u001b[39m ):\n\u001b[32m   1232\u001b[39m     warnings.warn(\n\u001b[32m   1233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal html to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_html\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1234\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Neural-Playground/.venv/lib/python3.12/site-packages/pandas/io/html.py:971\u001b[39m, in \u001b[36m_parse\u001b[39m\u001b[34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m retained = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m flav \u001b[38;5;129;01min\u001b[39;00m flavor:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m     parser = \u001b[43m_parser_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m     p = parser(\n\u001b[32m    973\u001b[39m         io,\n\u001b[32m    974\u001b[39m         compiled_match,\n\u001b[32m   (...)\u001b[39m\u001b[32m    979\u001b[39m         storage_options,\n\u001b[32m    980\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Neural-Playground/.venv/lib/python3.12/site-packages/pandas/io/html.py:918\u001b[39m, in \u001b[36m_parser_dispatch\u001b[39m\u001b[34m(flavor)\u001b[39m\n\u001b[32m    916\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mbs4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlxml.etree\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _valid_parsers[flavor]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/Neural-Playground/.venv/lib/python3.12/site-packages/pandas/compat/_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'lxml'.  Use pip or conda to install lxml."
     ]
    }
   ],
   "source": [
    "nba_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2936873-f34a-4b1a-bc0c-275d5844aae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
